{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding Analysis with scraped data from Indeed Australia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the dataset and drop douplicates \n",
    "data = pd.read_csv('DF_INDEED_CLEAN.csv', encoding='latin1')\n",
    "df = data.drop_duplicates()\n",
    "df = pd.DataFrame(df)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing and cleaning the data \n",
    "def cleanData(desc):\n",
    "    desc = word_tokenize(desc)\n",
    "    desc = [word.lower() for word in desc if word.isalpha() and len(word) > 2]\n",
    "    desc = [word for word in desc if word not in stop_words]\n",
    "    desc = [word for word in desc if word not in stop_words_add]\n",
    "    return desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get stopwords and define other stopwords if needed \n",
    "stop_words = stopwords.words('english')\n",
    "stop_words_add = ['Ã¢']\n",
    "stop_words.extend(stop_words_add)\n",
    "\n",
    "#apply to own data \n",
    "tags_df = df[\"Description\"].apply(cleanData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make seperate dataframe with only cleaned descriptions \n",
    "tags = pd.DataFrame(tags_df)\n",
    "tags = tags.rename(columns = {'Description': 'des_clean'}, inplace = False)\n",
    "#not a smart way to do this but it cleans it and adds new column \n",
    "df_tags = df.join(tags, how=\"outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating gender bias dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words_dic = {}\n",
    "def calculate_gender_bias_dictionary(df_column, w2vmodel, word1, word2):\n",
    "#df_column = descriptions, w2vmodel = our model of choice, word1/2 = gender identifiers \n",
    "        model = w2vmodel\n",
    "        male_word = word1\n",
    "        female_word = word2\n",
    "# Join all job descriptions\n",
    "        all_words = ' '.join(df_column)\n",
    "# Finds all unique words in the \"big word\"\n",
    "        unique_words = set(all_words.split(' '))\n",
    "# Create a dictionary with all unique words with gender bias values\n",
    "        for word in unique_words:\n",
    "            if word not in model.vocab.keys(): \n",
    "                unique_words_dic[word] = float(-1000.0) #assign -1000 if not in dictionary \n",
    "            else:\n",
    "                male_sim = float(w2vmodel.similarity(word, word1)) \n",
    "                female_sim = float(w2vmodel.similarity(word, word2)) \n",
    "                difference = male_sim - female_sim \n",
    "                unique_words_dic[word] = float(difference)\n",
    "        return unique_words_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stupid way of doing this, but had problems with the two different formats in the datasets \n",
    "df_kol = list(set([a for b in df_tags.des_clean.tolist() for a in b]))\n",
    "df_column = df_kol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify model to use \n",
    "w2vmodel = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True, limit=500000)\n",
    "#speficy gender identifiers \n",
    "word1 = \"man\"\n",
    "word2 = \"woman\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_bias_dict = calculate_gender_bias_dictionary(df_column, w2vmodel, word1, word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function that calculates for each job description \n",
    "def calculate_gender_bias(annonce, gender_bias_dict): \n",
    "    gender_bias_total = 0\n",
    "    avg_gender_bias = 0\n",
    "    count = 0\n",
    "    #list_words = annonce.split() \n",
    "    for word in annonce:\n",
    "        bias = gender_bias_dict[word] \n",
    "        if bias != -1000.0:\n",
    "            gender_bias_total += bias \n",
    "            count += 1\n",
    "    return float((gender_bias_total / count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after specifying the model and words we can calculate an avage score of all the words in each job description and append score to our dataframe  \n",
    "gender_bias = []\n",
    "for i in df_tags['des_clean']:\n",
    "    cal_bias = calculate_gender_bias(i, gender_bias_dict)\n",
    "    gender_bias.append(cal_bias)\n",
    "#appening column to our dataframe     \n",
    "df_tags[\"man_woman\"] = gender_bias\n",
    "#saving full dataframe as csv \n",
    "df_tags.to_csv(\"df_tags_28dec.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting gender bias score for each word in our gender bias dictionary \n",
    "pd_bias = pd.DataFrame.from_dict(gender_bias_dict, orient='index')\n",
    "#save csv with gender scores \n",
    "pd_bias.to_csv(\"gender_bias_dict.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing salary rates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_salary(row):\n",
    "    salary = row[\"Salary\"]\n",
    "    if \"-\" in salary:\n",
    "        split = salary.split(\"-\")\n",
    "        salary_min = split[0]\n",
    "        salary_max = split[1]\n",
    "    else:\n",
    "        salary_min = salary\n",
    "        salary_max = salary\n",
    "    row[\"salary_min\"] = salary_min.replace(\"$\",\"\").replace(\"a month\", \"\").replace(\"a year\",\"\").replace(\",\",\"\").replace(\"an hour\",\"\").replace(\",\",\"\").replace(\"a week\",\"\").replace(\",\",\"\")\n",
    "    row[\"salary_max\"] = salary_max.replace(\"$\",\"\").replace(\"a month\", \"\").replace(\"a year\",\"\").replace(\",\",\"\").replace(\"an hour\",\"\").replace(\",\",\"\").replace(\"a week\",\"\").replace(\",\",\"\")\n",
    "    \n",
    "    if \"month\" in row[\"Salary\"]:\n",
    "        row[\"salary_min\"] = float(row[\"salary_min\"])*12\n",
    "        row[\"salary_max\"] = float(row[\"salary_max\"])*12\n",
    "    if \"hour\" in row[\"Salary\"]:\n",
    "        row[\"salary_min\"] = float(row[\"salary_min\"])*1710\n",
    "        row[\"salary_max\"] = float(row[\"salary_max\"])*1710\n",
    "    if \"week\" in row[\"Salary\"]:\n",
    "        row[\"salary_min\"] = float(row[\"salary_min\"])*45\n",
    "        row[\"salary_max\"] = float(row[\"salary_max\"])*45       \n",
    "    \n",
    "    return row\n",
    "\n",
    "df_salary = data[data[\"Salary\"]!= \"None\"].dropna()\n",
    "df_salary = df_salary.apply(format_salary,axis=1)\n",
    "\n",
    "df_salary[\"salary_min\"] = pd.to_numeric(df_salary[\"salary_min\"],'coerce')\n",
    "df_salary[\"salary_max\"] = pd.to_numeric(df_salary[\"salary_max\"],'coerce')\n",
    "df_salary[\"salary_mean\"] = (df_salary[\"salary_min\"]+df_salary[\"salary_max\"])/2\n",
    "\n",
    "df_salary.to_csv(\"df_salary27dec.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = model_w2v[\"computer_programmer\"] - model_w2v[\"man\"] + model_w2v[\"woman\"]\n",
    "model_w2v.most_similar([vec])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
