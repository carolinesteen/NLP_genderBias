{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customized scraper for Australian indeed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd \n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium.common.exceptions import ElementClickInterceptedException, TimeoutException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "\n",
    "from lxml.html import fromstring\n",
    "import requests\n",
    "import urllib \n",
    "from itertools import cycle\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download chrome driver and put it in the same folder as script, and give it you path :-) \n",
    "driver = webdriver.Chrome('/Users/carolinetvergaard/Desktop/NLP/scraping/chromedriver')\n",
    "\n",
    "#define dataframe columns \n",
    "dataframe = pd.DataFrame(columns=[\"Title\",\"Location\",\"Company\",\"Salary\",\"Description\", \"Occupation\", \"Sector\"])\n",
    "\n",
    "for i in range(0,600,2): # this goes from 0 to n with increments of i (tried to change to avoid duplicates)\n",
    "    \n",
    "    driver.get(\"https://au.indeed.com/jobs?q=media&start=\"+str(i))\n",
    "    \n",
    "    driver.implicitly_wait(7)\n",
    "\n",
    "    all_jobs = driver.find_elements_by_class_name('result')\n",
    "    \n",
    "    for job in all_jobs:\n",
    "        result_html = job.get_attribute('innerHTML')\n",
    "        soup = BeautifulSoup(result_html,'html.parser')\n",
    "    try:\n",
    "        title = soup.find(\"a\",class_=\"jobtitle\").text.replace('\\n','')\n",
    "    except:\n",
    "        title = 'None'\n",
    "\n",
    "    try:\n",
    "        location = soup.find(class_=\"location\").text\n",
    "    except:\n",
    "        location = 'None'\n",
    "\n",
    "    try:\n",
    "        company = soup.find(class_=\"company\").text.replace(\"\\n\",\"\").strip()\n",
    "    except:\n",
    "        company = 'None'\n",
    "\n",
    "    try:\n",
    "        salary = soup.find(class_=\"salary\").text.replace(\"\\n\",\"\").strip()\n",
    "    except:\n",
    "        salary = 'None'\n",
    "    \n",
    "    sum_div = job.find_elements_by_class_name(\"summary\")[0]\n",
    "    \n",
    "    try:\n",
    "        sum_div.click()\n",
    "    except:\n",
    "        close_button = driver.find_elements_by_class_name(\"popover-x-button-close\")[0]\n",
    "        close_button.click()\n",
    "        sum_div.click()\n",
    "        \n",
    "    try:\n",
    "        job_desc = driver.find_element_by_id('vjs-desc').text\n",
    "    except:\n",
    "        job_desc = 'None'\n",
    "    occupation = \"Media Relations\" #edit\n",
    "    sector = \"Creative Arts & Design\" #edit \n",
    "\n",
    "    dataframe = dataframe.append({'Title':title,'Location':location,\"Company\":company,\"Salary\":salary,\"Description\":job_desc, \"Occupation\":occupation, \"Sector\":sector},ignore_index=True)\n",
    "\n",
    "    dataframe.to_csv(\"media.csv\",index=False) #edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attemp to change proxy to avoid capcha "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml.html import fromstring\n",
    "import requests\n",
    "from itertools import cycle\n",
    "import traceback\n",
    "\n",
    "def get_proxies():\n",
    "    url = 'https://free-proxy-list.net/'\n",
    "    response = requests.get(url)\n",
    "    parser = fromstring(response.text)\n",
    "    proxies = set()\n",
    "    for i in parser.xpath('//tbody/tr')[:10]:\n",
    "        if i.xpath('.//td[7][contains(text(),\"yes\")]'):\n",
    "            proxy = \":\".join([i.xpath('.//td[1]/text()')[0], i.xpath('.//td[2]/text()')[0]])\n",
    "            proxies.add(proxy)\n",
    "    return proxies\n",
    "\n",
    "\n",
    "#If you are copy pasting proxy ips, put in the list below\n",
    "#proxies = ['121.129.127.209:80', '124.41.215.238:45169', '185.93.3.123:8080', '194.182.64.67:3128', '106.0.38.174:8080', '163.172.175.210:3128', '13.92.196.150:8080']\n",
    "proxies = get_proxies()\n",
    "proxy_pool = cycle(proxies)\n",
    "\n",
    "url = 'https://httpbin.org/ip'\n",
    "for i in range(1,11):\n",
    "    #Get a proxy from the pool\n",
    "    proxy = next(proxy_pool)\n",
    "    print(\"Request #%d\"%i)\n",
    "    try:\n",
    "        response = requests.get(url,proxies={\"http\": proxy, \"https\": proxy})\n",
    "        print(response.json())\n",
    "    except:\n",
    "        #Most free proxies will often get connection errors. You will have retry the entire request using another proxy to work. \n",
    "        #We will just skip retries as its beyond the scope of this tutorial and we are only downloading a single url \n",
    "        print(\"Skipping. Connnection error\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
